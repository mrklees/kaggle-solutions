{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "d8344173-ca54-4899-85f3-b5832847420d",
        "_uuid": "5fcee9b3667cea01280766912f3bf782b5e9bd8e"
      },
      "cell_type": "markdown",
      "source": "# A Deep Learning Approach to the Titanic Data Set\n\n*Dec 2017 Update* I notice that a few folks have forked this kernal, and so wanted to provide an update on how I'm developing models these days.  I will utilize this kernal to build neural networks using both the scikit-learn library as well as Keras to highlight strategies like ensambling many networks together.  Feel free to leave a comment if you have any questions!  I'll try to answer as best I can.  Also, if you find this kernal helpful **please upvote** so that others can find this resource."
    },
    {
      "metadata": {
        "_cell_guid": "f333d0fa-c710-48ac-bf7a-81e8bda06533",
        "_execution_state": "idle",
        "_uuid": "0623842dd2852d12afd9b1724fb4b26edc0d8092",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Data Structures\nimport pandas as pd\nimport numpy as np\n\n# Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom random import randint\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import VotingClassifier, BaggingClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import LabelBinarizer, StandardScaler",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "396fe37d-d090-435c-8631-23f0184b6ad7",
        "_uuid": "7698a7435ed4acb303b98a248ffa24e2767f7d1d"
      },
      "cell_type": "markdown",
      "source": "What follows are a couple classes that I've defined to encapsulate my data pipeline and scikit-learn model.  I've added some documentation throughout"
    },
    {
      "metadata": {
        "_cell_guid": "3478b5ab-bf85-469f-afe3-3b18d79b9290",
        "_execution_state": "idle",
        "_uuid": "4dc516b5462a6e01e8e11adaa5a82ab4fe683c40",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "class TitanicData(object):\n    \"\"\"Titanic Data\n    \n    This class will contain the entire data pipeline from raw data to prepared \n    numpy arrays.  It's eventually inherited by the model class, but is left \n    distinct for readbility and logical organization.\n    \"\"\"\n    filepath = \"../input/\"\n    train_fn = 'train.csv'\n    test_fn = 'test.csv'\n    \n    def __init__(self):\n        self.X_train, self.y_train, self.X_valid, self.y_valid = self.preproc()\n    \n    def import_and_split_data(self):\n        \"\"\"Import that data and then split it into train/test sets.\n        \n        Make sure to stratify.  This is often not even enough, but will get you closer \n        to having your validation score match kaggles score.\"\"\"\n        X, y = self.select_features(pd.read_csv(self.filepath + self.train_fn))\n        X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.25, random_state = 606, stratify = y)\n        return X_train, y_train, X_valid, y_valid\n        \n    def select_features(self, data):\n        \"\"\"Selects the features that we'll use in the model. Drops unused features\"\"\"\n        target = ['Survived']\n        features = ['Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n           'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n        dropped_features = ['Cabin', 'Ticket']\n        X = data[features].drop(dropped_features, axis=1)\n        y = data[target]\n        return X, y\n    \n    def fix_na(self, data):\n        \"\"\"Fill na's with the mean (in the case of fare), and with C in the case of embarked\"\"\"\n        na_vars = {\"Fare\" : data.Fare.median(), \"Embarked\" : \"C\", \"Age\" : data.Age.median()}\n        return data.fillna(na_vars)\n    \n    def create_dummies(self, data, cat_vars, cat_types):\n        \"\"\"Processes categorical data into dummy vars\"\"\"\n        cat_data = data[cat_vars].values\n        for i in range(len(cat_vars)):   \n            bins = LabelBinarizer().fit_transform(cat_data[:, 0].astype(cat_types[i]))\n            cat_data = np.delete(cat_data, 0, axis=1)\n            cat_data = np.column_stack((cat_data, bins))\n        return cat_data\n       \n    def standardize(self, data, real_vars):\n        \"\"\"Processes numeric data\"\"\"\n        real_data = data[real_vars]\n        scale = StandardScaler()\n        return scale.fit_transform(real_data)\n    \n    def extract_titles(self, data):\n        \"\"\"Extract titles from the Name field and create appropriate One Hot Encoded Columns\"\"\"\n        title_array = data.Name\n        first_names = title_array.str.rsplit(', ', expand=True, n=1)\n        titles = first_names[1].str.rsplit('.', expand=True, n=1)\n        known_titles = ['Mr', 'Mrs', 'Miss', 'Master', 'Don', 'Rev', 'Dr', 'Mme', 'Ms',\n           'Major', 'Lady', 'Sir', 'Mlle', 'Col', 'Capt', 'the Countess',\n           'Jonkheer']\n        for title in known_titles:\n            try:\n                titles[title] = titles[0].str.contains(title).astype('int')\n            except:\n                titles[title] = 0\n        return titles.drop([0,1], axis=1).values\n    \n    def engineer_features(self, dataset):\n        dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n        dataset['IsAlone'] = 1 #initialize to yes/1 is alone\n        dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 #the rest are 0\n        return dataset\n    \n    def preproc(self):\n        \"\"\"Executes the full preprocessing pipeline.\"\"\"\n        # Import Data & Split\n        X_train_, y_train, X_valid_, y_valid = self.import_and_split_data()\n        # Fill NAs\n        X_train, X_valid = self.fix_na(X_train_), self.fix_na(X_valid_)\n        # Feature Engineering\n        X_train, X_valid = self.engineer_features(X_train), self.engineer_features(X_valid)\n        \n        # Preproc Categorical Vars\n        cat_vars = ['Pclass', 'Sex', 'Embarked']\n        cat_types = ['int', 'str', 'str']\n        X_train_cat, X_valid_cat = self.create_dummies(X_train, cat_vars, cat_types), self.create_dummies(X_valid, cat_vars, cat_types)\n        \n        # Extract Titles\n        X_train_titles, X_valid_titles = self.extract_titles(X_train), self.extract_titles(X_valid)\n        \n        # Preprocess Numeric Vars\n        real_vars = ['Fare', 'SibSp', 'Parch', \"FamilySize\", \"IsAlone\"]\n        X_train_real, X_valid_real = self.standardize(X_train, real_vars), self.standardize(X_valid, real_vars)\n        \n        # Recombine\n        X_train, X_valid = np.column_stack((X_train_cat, X_train_real, X_train_titles)), np.column_stack((X_valid_cat, X_valid_real, X_valid_titles))\n        \n        return X_train.astype('float32'), y_train.values, X_valid.astype('float32'), y_valid.values\n\n    def preproc_test(self):\n        test = pd.read_csv(self.filepath + self.test_fn)\n        labels = test.PassengerId.values\n        test = self.fix_na(test)\n        test = self.engineer_features(test)\n        # Preproc Categorical Vars\n        cat_vars = ['Pclass', 'Sex', 'Embarked']\n        cat_types = ['int', 'str', 'str']\n        test_cat = self.create_dummies(test, cat_vars, cat_types)\n        \n        # Extract Titles\n        test_titles = self.extract_titles(test)\n        \n        # Preprocess Numeric Vars\n        real_vars = ['Fare', 'SibSp', 'Parch', \"FamilySize\", \"IsAlone\"]\n        test_real = self.standardize(test, real_vars)\n        \n        # Recombine\n        test = np.column_stack((test_cat, test_real, test_titles))\n        return labels, test\n        \nclass TitanicModel(TitanicData):\n    \n    def __init__(self):\n        self.X_train, self.y_train, self.X_valid, self.y_valid = self.preproc()\n        \n    def build_single_model(self, random_state, num_layers, verbose=False):\n        \"\"\"Create a single neural network with variable layers\n        \n        This function will both assign the model to the self.model attribute, as well\n        as return the model.  I'm pretty afraid of side effects resulting from \n        changing the state within the object, but then it hasn't ruined by day yet...\n        \"\"\"\n        model = MLPClassifier(\n                        hidden_layer_sizes=(1024, ) * num_layers,\n                        activation='relu',\n                        solver='adam',\n                        alpha=0.0001,\n                        batch_size=100,\n                        max_iter=64,\n                        learning_rate_init=0.001,\n                        random_state=random_state,\n                        early_stopping=True,\n                        verbose=verbose\n                        )\n        self.model = model\n        return model\n    \n    def fit(self):\n        \"\"\"Fit the model to the training data\"\"\"\n        self.model.fit(self.X_train, self.y_train)\n        \n    def evaluate_model(self):\n        \"\"\"Score the model against the validation data\"\"\"\n        return self.model.score(self.X_valid, self.y_valid)\n        \n    def build_voting_model(self, model_size=10, n_jobs=1):\n        \"\"\"Build a basic Voting ensamble of neural networks with various seeds and numbers of layers\n        \n        The idea is that we'll generate a large number of neural networks with various depths \n        and then aggregate across their beliefs.\n        \"\"\"\n        models = [(str(seed), self.build_single_model(seed, randint(2, 15))) for seed in np.random.randint(1, 1e6, size=model_size)]\n        ensamble = VotingClassifier(models, voting='soft', n_jobs=n_jobs)\n        self.model = ensamble\n        return ensamble\n    \n    def prepare_submission(self, name):\n        labels, test_data = self.preproc_test()\n        predictions = self.model.predict(test_data)\n        subm = pd.DataFrame(np.column_stack([labels, predictions]), columns=['PassengerId', 'Survived'])\n        subm.to_csv(\"{}.csv\".format(name), index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "74c9fb2a-6836-4b0f-b522-db5329c55e42",
        "_uuid": "00fd6c011e5bd7972f80131c704e4212107ad6ce"
      },
      "cell_type": "markdown",
      "source": "The rest of this workbook is what my typical Jupyter notebooks look like. Note that I'm not going to spend any time on exploratory data analysis.  There are lots of great kernals with exploratory visualization of this dataset, much of which I have referenced to do the feature engineering above.  "
    },
    {
      "metadata": {
        "_cell_guid": "2688fd7a-d269-4667-b20a-c28e2a48c48c",
        "_execution_state": "idle",
        "_uuid": "596cd38828f9c244f2024d4e9b4857cc7846fadf",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nmodel = TitanicModel()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "1ae02488-cea1-4663-bd7b-757c32e8fa12",
        "_uuid": "41515bef40b9c52ddaea12e616a62bd955bd35e0"
      },
      "cell_type": "markdown",
      "source": "Lets create a basic neural network using our class and fit it. "
    },
    {
      "metadata": {
        "_cell_guid": "4e12681a-1fd7-41ec-aa98-45f2fbf55f46",
        "_uuid": "aadb6d0c6ae7860a663b4f89523efaad05bd7b2c",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "model.build_single_model(num_layers=4, random_state=606, verbose=True)\nmodel.fit()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "56acf225-7b54-40af-9e4b-e9942f6ea304",
        "_uuid": "9ff4a19ab882aa2d6a865c3855ed6670b79bef6b"
      },
      "cell_type": "markdown",
      "source": "We can then score our model against our reserved dataset.  Note that the validation score it refers to above is actually calculated on 10% of the training set, not the validation set. "
    },
    {
      "metadata": {
        "_cell_guid": "9db3754b-4284-4aea-8358-74cd807e6780",
        "_uuid": "fbba9390a3230253f67dde43099e9ff8810118a7",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "model.evaluate_model()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "0f99e1f0-028a-4e30-bc5d-c6edcdf383ac",
        "_uuid": "de8e6a383fba2a7de85da332b560431ab50713c4",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "#model.prepare_submission('simplenn')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "9654d87f-c5c7-435a-8a0d-3f8e7c3197d7",
        "_uuid": "456899ff1b0f1666c1794eaa3b0503f5d3ef6a94"
      },
      "cell_type": "markdown",
      "source": "Thanks to the code we've already written, creating an ensamble out of these single models isn't too challenging.  Let's start with the ensamble voting classifier provided by scikit-learn.  This will let us create a nice ensamble with various numbers of layers and seeds to try to find something better."
    },
    {
      "metadata": {
        "_cell_guid": "38e226f1-59ea-42e2-b943-d89767fbc8f5",
        "_uuid": "8010d91f90abafdb636111f3406b310abd719307",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "voting = model.build_voting_model(model_size=10, n_jobs=4)\n#model.fit()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "053989c6-6424-45e6-8f53-4bc535ed979d",
        "_uuid": "49f0c3dbb0ef98cb4122fc8f1c9f7b40ca945a5b",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "#model.evaluate_model()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "3611c522-c32a-4751-a2b0-c48d4c7c5e6e",
        "_uuid": "035dfb20b9dda0ddead295b45c57e94a38caf144",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "#model.prepare_submission('ensambled_nn')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "26786031-e2f6-42b6-9ab8-eb940561ccf2",
        "_uuid": "d66bf4866f94fe635d76c4adc1a4a82d8b09bb53"
      },
      "cell_type": "markdown",
      "source": "Utilizing the ensamble only gave me ~1% improved accuracy on the validation set, but this carried into submission ,\n\nSo how could we do better?  While scikit-learn is super convenient for quickly building neural networks, there are some clear limitations.  For example, scikit-learn still doesn't have a production implementation of dropout, which is currently one of the preferred methods of neural network regularization.  With dropout we might be able to train deeper networks without worry about overfitting as much. So lets do it!"
    },
    {
      "metadata": {
        "_cell_guid": "af0f70af-5756-4d12-b93c-c2cba470246b",
        "_uuid": "93528834a3f0965162eada73c35948ab303906b9",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b37a4d0d-6e80-4ba8-b039-208a186f4e32",
        "_uuid": "e16f9e7c292177e34c8f775cd2be51f9fe149b6a",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "class TitanicKeras(TitanicData):\n    \n    def __init__(self):\n        self.X_train, self.y_train, self.X_valid, self.y_valid = self.preproc()\n        self.y_train, self.y_valid = to_categorical(self.y_train), to_categorical(self.y_valid)\n        self.history = []\n        \n    def build_model(self):\n        model = Sequential()\n        model.add(Dense(2056, input_shape=(29,), activation='relu'))\n        model.add(Dropout(0.1))\n        model.add(Dense(1028, activation='relu'))\n        model.add(Dropout(0.2))\n        model.add(Dense(1028, activation='relu'))\n        model.add(Dropout(0.3))\n        model.add(Dense(512, activation='relu'))\n        model.add(Dropout(0.4))\n        model.add(Dense(2, activation='sigmoid'))\n        model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n        self.model = model\n        \n    def fit(self, lr=0.001, epochs=1):\n        self.model.optimizer.lr = lr\n        hist = self.model.fit(self.X_train, self.y_train,\n                      batch_size=32, epochs=epochs,\n                      verbose=1, validation_data=(self.X_valid, self.y_valid),\n                      )\n        self.history.append(hist)\n        \n    def prepare_submission(self, name):\n        labels, test_data = self.preproc_test()\n        predictions = self.model.predict(test_data)\n        subm = pd.DataFrame(np.column_stack([labels, np.around(predictions[:, 1])]).astype('int32'), columns=['PassengerId', 'Survived'])\n        subm.to_csv(\"{}.csv\".format(name), index=False)\n        return subm",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "bdab76e8-4939-45f2-abcc-436067083bd4",
        "_uuid": "6fca9085fe15bcf55682489c69ff407098e8ecb5",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "model = TitanicKeras()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ab72b839-b509-424e-9bdd-7f6b1242189e",
        "_uuid": "aecdc8d8cc263f2ae31824d98134171b3a1ab4ac",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "model.build_model()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "57e342c5-1e1b-42b8-94df-b7080f63e9e3",
        "_uuid": "88464df61837d236bb27edb18839f9a5c1ce29c7",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "model.fit(lr=0.01, epochs=5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "087342c3-3855-4083-a4fb-2749245936d0",
        "_uuid": "1289763540506087147d66b0a125ce1a85b06cd8",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "model.fit(lr=0.001, epochs=10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "a2e92d33-0bd6-4500-b913-e92232330694",
        "_uuid": "566fe975c4bf81cdaf73311433ff6c34b0cd9f26",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "model.prepare_submission('keras')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ccd75e88-67a8-482c-b50f-d993fb4ba91e",
        "_uuid": "fdf6a6d2d9c08a47c777c959657f3881186d4a54",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "nbconvert_exporter": "python",
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.6.3",
      "pygments_lexer": "ipython3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}